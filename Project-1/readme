Description:

    1. Given an article and a list of keys, calculate the occurence of each key (UTF-8 encoding).

    2. If there are duplicate keys, ignore them.

    3. If the keys overlap each other, apply longest match.

    4. There might be million keys in list, and each of them is 6 ~ 21 bytes long.

    5. The article might be huge.

Method:

    1. Use Rabin-Karp rolling hash algorithm for the initial string comparison ( compare the hash values ). -- constant time

    2. Use the identical hash value as the index and find the corresponding bucket,  -- O(1)

    3. Traverse the linkedlist and do byte-to-byte comparison on each node to see if the key is actual in the table. -- constant time (depends on collision)

Details:

    1. Base for Rabin-Karp rolling hash == 31, since it has good hash distribution, and so are 29, 33, 127. However, there are a lot of famous hash functions

       that use 31 as bases, so I decide to follow the track.

    2. Modulor for Rabin-Karp rolling hash == power of 2, in this case, 2^22. We all know modulor is a costy operations, and when implementing Rabin-Karp

       rolling hash, there might be a good chance to constantly doing modulor and abs. We can simply use a modulor that is power of 2, in this case,

       times of modulor and abs operations can be replaced just by one bitwise and operation.


Notes:

    I used to build up a look-up table when implementing Rabin-Karp Rolling hash, which costs a lot of memories.

    After this project I can finally realize that a look-up table is not necessary all the time, it is case by case.

    When you want to get the hash value of a random subset of the string, building up a table saves a lot of time,

    but if you don't (for example, if you have a fixed length of string to search), then a table would be a disaster.


    C strstr function is really fast implementation for finding occurence for a given substring, the algorithm lies behind is KMP algorithm,

    strstr might be fast when doing single pattern searching, but in this project, it takes forever to finish calculating the occurence of millions of words.





