Desciption:

              1. Given an article and a list of keys, calculate the occurence of each key.
              2. If there are duplicated keys, ignore them.
              3. If the strings overlapped each other, apply longest match.
              4. There might be millions of keys, and each of them is 6 to 21 bytes long.
              5. The article might be huge.

Thoughts:

              1. Use Rabin-Karp rolling hash to compare if the strings match at the first place.
              2. Use the identical hash value as the table index to find the corresponded buckets.
              3. If the key exists, add the counter, if not, add the key to the table.

Details:

              1. Rabin-Karp rolling hash base: In this project, I use 31 as the base since it has
                    good hash distributions, but since there are famous divide hash functions select
                    31 as their bases, I decide to follow this trend.


              2. Hash modulor (which indicates the size of the hash table): I have done some tests
                    on 'how the hash base and the size of the hash table influence the collision rate',
                    The result indicates that the size of the table seems to be the only factor that has
                    significant and direct influence toward the collision rate. On the other hand, the bigger
                    the hash table is, the less collision will occur, however, the memory cost is huge at the time.

                    In this project, I select 2^22 as the table size since there might be millions of keys, but
                    there is another reason for that. One of the good things about the Rabin-Karp algorithms is we
                    can find the next hash value by using the previous one, in this case, we might constantly do
                    the modulor and abs operations, however, if our size is a power of 2, we can simply replace
                    the duplicating operations with just 1 bitwise& operation, this speeds up a lot before the
                    compiler optimizations.

Notes:

              1. I used to build a look-up table every time I implement Rabin-Karp rolling hash, however, it is
                    not always necessary to do so.

                    When you need to get the hash value of any random substring in O(1), building up a talbe might help,
                    but if you don't, for example, if you have a fixed length of a string, this table will be nothing but
                    a waste.

              2. C string::strstr is a really fast implementation for single pattern searching, the algorithm lies behind is
                    the famous KMP algorithm. However, it will take forever to finish calculating the occurence for hundreds of
                    thousands of keys.

Execution:

            1. Given 800 MB article and 80,000 keys, it takes 37 ~ 40 seconds to finish calculating.


